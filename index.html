<html>
<!-- PLEASE CREDIT https://prashnani.github.io if using this template. See the end of this document to for more information. -->
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">	 
  	<meta name="description" content="Ekta Prashnani Homepage">
  	<meta name="keywords" content="Ekta Prashnani">
  	<meta name="author" content="Ekta Prashnani">
  	<meta name="viewport" content="width=device-width, initial-scale=1.0">	
	<title>Ekta Prashnani Homepage </title>
	<style type="text/css"></style>
</head>

<body>
	<table border="0" width="45%" align="center" style="max-width:100%">
	<tbody><tr><td></td><td valign="top">

        <br>
        <table style="font-size: 13pt;" border="0" width="100%" bgcolor="F5F5F5">
            <tbody><tr>
                <td width="50%" align="center">                    
                    <img width="200" src="indexfiles/profile.png" border="5%" style="border-color:transparent" >
                </td>
                <td align="left">
                    <font face= "Century Gothic, CenturyGothic, AppleGothic, sans-serif" size="6"> 
                        Ekta Prashnani<br>
                    </font>
                    <font face="Century Gothic, CenturyGothic, AppleGothic, sans-serif" size="4"> 
                        Research Scientist <br>
                        NVIDIA<br>                        
                        <!-- [<a href="https://github.com/prashnani" border="0" style="color:528B8B">GitHub</a>]
                        [<a href="https://scholar.google.com/citations?user=na-Ba1EAAAAJ&hl=en" border="0" style="color:528B8B">Google Scholar</a>]
						[<a href="https://prashnani.github.io/indexfiles/Ekta_Prashnani_CV_NOV2022.pdf" border="0" style="color:528B8B">CV</a>]<br> -->
                    </font>
                </td>
            </tr>
        </tbody></table> 
        <br>
        <p>
       
        <font style="font-size: 13pt;" face="Century Gothic, CenturyGothic, AppleGothic, sans-serif" style="color:528B8B" target="_blank">
		I am a Research Scientist at NVIDIA Research, working on verifying trustworthy use of synthetic content (see <a href="https://youtu.be/mjuQqwDOyQE">Avatar Fingerprinting</a> for verifying the authorized use of synthetic talking-head videos at ECCV'24) and on multi-modal foundation models.
		
		<br>

		Some past notable works include <a href="https://arxiv.org/abs/2211.09363">PhaseForenics</a>, a generalizable, phase-based deepfake detector; <a href="https://arxiv.org/abs/1806.02067" style="color:528B8B" target="_blank">PieAPP</a> a perceptually-consistent image error assessment method and accompanying dataset; and 
		a <a href="https://arxiv.org/abs/2104.08038" style="color:528B8B" target="_blank">noise-aware training</a> approach for video saliency.
		<br>
		<br>		
		</font>

        </p><hr size="1" align="left" color="c3c3c3">     		
		<font face= "Century Gothic, CenturyGothic, AppleGothic, sans-serif" style="font-size: 22pt;"> News
        <table style="font-size: 13pt;" border="0" width="100%" font size="3">
            <tbody>
				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>		    		
					<tr><td width="100%" align="Left">Jul, 2024: <a href="https://research.nvidia.com/labs/nxp/avatar-fingerprinting/">Avatar Fingerprinting</a>, the first work on verifying authorized use of synthetic talking-head videos, is accepted for publication at ECCV (summary video <a href="https://youtu.be/mjuQqwDOyQE">link</a>).</td></tr><tr></tr><tr></tr>
					<tr><td width="100%" align="Left">Jun, 2024: PhaseForensics (a phase-based deepfake detector) is accepted for publication at the Transactions on Image Processing: stay tuned for updates!</td></tr><tr></tr><tr></tr>
					<tr><td width="100%" align="Left">Jan, 2024: <a href="https://scholar.google.com/citations?user=jJz3mXcAAAAJ&hl=en">Leonard Salewski</a> started his internship with me: an exciting start to 2024!</td></tr><tr></tr><tr></tr>
					<tr><td width="100%" align="Left">Sep, 2023: Our work on Avatar Fingerprinting for verifying the authorized use of synthetic talking-head videos is now available on <a href="https://arxiv.org/abs/2305.03713">arXiv</a>.</td></tr><tr></tr><tr></tr>
					<tr><td width="100%" align="Left">Nov, 2022: Our work on PhaseForensics: A generalizable deepfake detector for phase-based motion analysis is now available on <a href="https://arxiv.org/abs/2211.09363">arXiv</a>.</td></tr><tr></tr><tr></tr>					
					<tr><td width="100%" align="Left">Apr, 2022: Started as a Research Scientist in the Human Performance and Experience team at NVIDIA.</td></tr><tr></tr><tr></tr>
		    		<tr><td width="100%" align="Left">Feb, 2022: Successfully defended my thesis "Data-driven Methods for Evaluating the Perceptual Quality and Authenticity of Visual Media".</td></tr><tr></tr><tr></tr>
		    		<tr><td width="100%" align="Left">Oct, 2021: Participated in ICCV 2021 Doctoral Consortium.</td></tr><tr></tr><tr></tr>
		    		<tr><td width="100%" align="Left">Jun, 2021: ECE Dissertation Fellowship by the ECE department at UCSB.</td></tr><tr></tr><tr></tr>
		    		<!-- <tr><td width="100%" align="Left">Apr, 2021: Our work on noise-aware training strategies for visual saliency prediction (accepted at BMVC'21) is now available on <a href="https://arxiv.org/abs/2104.08038" style="color:528B8B" target="_blank">arXiv</a>.</td></tr><tr></tr><tr></tr>
		    		<tr><td width="100%" align="Left">Jan, 2021: The PieAPP dataset is now available publicly! See <a href="https://github.com/prashnani/PerceptualImageError/blob/master/dataset/dataset_README.md" style="color:528B8B" target="_blank">
					here</a> for details. </td></tr><tr></tr><tr></tr>
					<tr><td width="100%" align="Left">Jun, 2018: Our CVPR2018 paper about a new <a href="https://arxiv.org/abs/1806.02067" style="color:528B8B" target="_blank">
					perceptual image-error metric (PieAPPv0.1)</a> 
					and the associated <a href="https://github.com/prashnani/PerceptualImageError" style="color:528B8B" target="_blank">source code and trained model</a> 
					is now online!</td></tr><tr></tr><tr></tr> -->
			</tbody>
		</table> 
        <hr size="1" align="left" color="c3c3c3">   		
		
		<font face= "Century Gothic, CenturyGothic, AppleGothic, sans-serif" style="font-size: 22pt;"> Publications
		<table style="font-size: 13pt;" border="0" width="100%" font size="3">
            <tbody>

				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<tr><td width="100%" align="Left">Ekta Prashnani, Koki Nagano, Shalini De Mello, David Leubke, Orazio Gallo, "Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos" <em> European Conference on Computer Vision, 2024.</em> <br> <center>
					[<a href="https://arxiv.org/abs/2305.03713" style="color:528B8B">paper</a>]														
					[<a href="https://research.nvidia.com/labs/nxp/avatar-fingerprinting/">project page</a>]
					[<a href="https://research.nvidia.com/labs/nxp/nvfair/">project page</a>]
					[<a href="https://research.nvidia.com/labs/nxp/avatar-fingerprinting/">summary video</a>]</center>
				<p><center>
				<img width="60%" src="indexfiles/avatar-fingerprinting-teaser.png" border="0" ></center>
				</p>				
				</td></tr>

				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<tr><td width="30%" align="Left">Ekta Prashnani, Michael Goebel, B. S. Manjunath, "Generalizable Deepfake Detection with Phase-Based Motion Analysis" <em> IEEE Transactions on Image Processing, 2024.</em> <br> <center>
					[<a href="https://arxiv.org/abs/2211.09363" style="color:528B8B">paper</a>]														
					[source code (Coming soon!)]					
					[summary video (Coming soon!)]</center>
				<p><center>
				<img width="100%" src="indexfiles/phase-forensics-teaser.png" border="0" ></center>
				</p>				
				</td></tr>

				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<tr><td width="100%" align="Left">Ekta Prashnani, Orazio Gallo, Joohwan Kim, Josef Spjut, Pradeep Sen, Iuri Frosio, "Noise-Aware Video Saliency Prediction," <em> British Machine Vision Conference, 2021.</em> <br> <center>
					[<a href="https://arxiv.org/abs/2104.08038" style="color:528B8B">paper</a>]														
					[<a href="https://github.com/NVlabs/NAT-saliency" style="color:528B8B">source code</a>]
					[<a href="https://github.com/NVlabs/NAT-saliency" style="color:528B8B">dataset</a>]					
					[<a href="https://youtu.be/KU4KTlKbH54" style="color:528B8B">summary video</a>]</center>
				<p><center>
				<img width="100%" src="indexfiles/NAT-overview.png" border="0" ></center>
				</p>
				
				</td></tr>
		    
		    <tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<tr><td width="100%" align="Left">Ekta Prashnani*, Herbert (Hong) Cai, Yasamin Mostofi and Pradeep Sen, "PieAPP: Perceptual Image-Error 
					Assessment through Pairwise Preference," <em> Computer Vision and Pattern Recognition, 2018.</em> <br> <center>
					[<a href="http://civc.ucsb.edu/graphics/Papers/CVPR2018_PieAPP/" style="color:528B8B">project webpage</a>] 
					[<a href="https://arxiv.org/abs/1806.02067v1" style="color:528B8B">paper</a>]														
					[<a href="https://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3483-supp.pdf" style="color:528B8B">supplementary</a>]					
					[<a href="https://prashnani.github.io/indexfiles/Prashnani_CVPR_2018_PieAPP_poster.pdf" style="color:528B8B">poster</a>]					
					[<a href="https://github.com/prashnani/PerceptualImageError" style="color:528B8B">source code</a>]
					[<a href="https://github.com/prashnani/PerceptualImageError/blob/master/dataset/dataset_README.md" style="color:528B8B">dataset</a>]					
					[<a href="https://www.ece.ucsb.edu/~ekta/projects/PieAPPv0.1/PieAPPv0.1_win64_exe.zip" style="color:528B8B">.exe</a>]</center>
				<p><center>
				<img width="100%" src="indexfiles/teaser_PieAPPv0.1.png" border="0" ></center>
				</p>
				
				</td></tr>
				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<tr><td width="100%" align="Left">Ekta Prashnani, Maneli Noorkami, Daniel Vaquero and Pradeep Sen, "A Phase-Based Approach for Animating Images Using Video Examples," <em> Computer Graphics Forum, August 2016, Volume 36,
Issue 6.</em> 
				<center>[<a href="http://onlinelibrary.wiley.com/doi/10.1111/cgf.12940/abstract"  style="color:528B8B" target="_blank">paper</a>] 
				[<a href="https://youtu.be/qaItuwdh1Rw"  style="color:528B8B" target="_blank">video results</a>] </center>
				</td></tr>
				<tr></tr>
				<tr></tr>
				<tr><td width="100%" align="Left"><small>*joint first authors</td></tr>
			</tbody>
		</table> 
        <!-- <hr size="1" align="left" color="c3c3c3">     -->

		<!-- <font face= "Century Gothic, CenturyGothic, AppleGothic, sans-serif" style="font-size: 22pt;" id="teaching"> Teaching 
		<table style="font-size: 13pt;" border="0" width="100%" font size="3">
            <tbody>
				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<td width="40%" align="center">                    
                    <img width="80%" src="indexfiles/capstone2019.jpg" border="0" >
                </td>
				<td align="Left">Technical Mentorship for EE Capstone (2018-2019)<br>
				<br><font style="font-size: 11pt;">Provided technical mentorship to seniors on their 
					EE Capstone projects (total five capstone projects).
<br>I worked very closely with the capstone team (left to right in the top picture: Benjamin Hirt, Erik Rosten, Shan-Wei Sun) working on  
<a href="https://capstone.engineering.ucsb.edu/projects/arthrex-autoscribe"  style="color:528B8B" target="_blank">the design and deployment of machine-learning-based 
medical image recognition algorithms for arthroscopic images</a> (sponsored by Arthrex). The team used an Nvidia Jetson TX2 interfaced with the live camera feed from 
an Arthrex surgical drawer, to deploy the trained deep-learning-based models. Check out a brief live demo 
<a href="https://www.youtube.com/watch?time_continue=1&v=U-643GFKFdY&feature=emb_logo"  style="color:528B8B" target="_blank">here</a> (starting at 25s). </font></td></tr>
			</tbody>
		</table>
		<br>			
		<table style="font-size: 13pt;" border="0" width="100%" font size="3">
            <tbody>
				<tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<td width="40%" align="center">                    
                    <img width="80%" src="indexfiles/capstone_2017_2018.jpg" border="0" >
                </td>
				<td align="Left">Technical Mentorship for EE Capstone (2017-2018)<br>
				<br><font style="font-size: 11pt;">Provided technical mentorship to seniors on their 
					EE Capstone projects (total five capstone projects).
<br>I worked very closely with the capstone team working on  
<a href="https://capstone.engineering.ucsb.edu/projects/arthrex-arthroscout"  style="color:528B8B" target="_blank">medical image recognition 
for arthroscopic images</a> (sponsored by Arthrex). The team (left to right: Jonathan Huynh, 
Phanitta Chomsinsap, Jacob Kurtz and Alae Amara) was selected to present their 
work at the <a href="https://capstone.engineering.ucsb.edu/events/edx2018"  style="color:528B8B" target="_blank">Engineering Design Expo, 2018</a>, at UCSB.</font></td></tr>
			</tbody>
		</table>
		<br>
		<table style="font-size: 13pt;" border="0" width="100%" font size="3">
            <tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<td width="40%" align="center">                    
                    <img width="90%" src="indexfiles/summer_mentorship_2017.png" border="0" >
                </td>
				<td align="Left">Research Mentor for High School Students (July 2017)<br> 
				<br><font style="font-size: 11pt;"> In the Summer of 2017, I had the opportunity to mentor 
				four exceptional high school students (left to right in first picture: Sohini Kar, Jungwoo Park, Joshua Doolan, James Wang)
				as a part of the <a href="http://www.summer.ucsb.edu/pre-college/research-mentorship-program-rmp"  style="color:528B8B" target="_blank">
				Summer Research Mentorship Program at UCSB</a>. 
				<br>I spent the first few weeks of the program teaching relevant concepts of computer vision and 
				machine learning to these students (they followed along easily - the age for brilliance keeps getting younger!). 
				The students spent the latter half of the program working on the research tasks I designed for them 
				in applying deep learning to object detection (for Sohini and Jungwoo) 
				and image restoration (for Joshua and James).</font></td></tr>
			</tbody>
		</table>
		<br>		
		<table style="font-size: 13pt;" border="0" width="100%" font size="3">
            <tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr>
				<td width="40%" align="center">                    
                    <img width="80%" src="indexfiles/capstone_2016_2017.jpg" border="0" >
                </td>
				<td align="Left">Technical Mentorship for EE Capstone (2016-2017)<br> 
				<br><font style="font-size: 11pt;">Provided technical mentorship to seniors on their EE Capstone projects (total six capstone projects).
I worked very closely with the capstone team working on 
<a href="https://capstone.engineering.ucsb.edu/projects/flir-pxtal	"  style="color:528B8B" target="_blank">deep-learning-based image super-resolution</a> (sponsored by Flir). 
<br>The team (left to right: Julian Castro, Connor Northend, Jose Jimenez) ended 
up winning the award for the Best Technical Capstone Project!</font></td></tr>
			</tbody>
		</table>
		<br>
		 -->
		
		<hr size="1" align="left" color="c3c3c3"><center>
		<font style="font-size: 11pt;">
		I can also be found on: <a href='https://www.linkedin.com/in/ektaprashnani/' style="color:528B8B" target="_blank">[LinkedIn]</a>
		<a href='https://github.com/prashnani' style="color:528B8B" target="_blank">[Github]</a>
		<a href='https://scholar.google.com/citations?user=na-Ba1EAAAAJ&hl=en' style="color:528B8B" target="_blank">[Google Scholar]</a>
		<a href='https://www.flickr.com/photos/133729196@N03/' style="color:528B8B" target="_blank">[Flickr]</a>
		<a href='https://twitter.com/ekta_prashnani' style="color:528B8B" target="_blank">[Twitter]</a>
		</font></center>
		<hr size="1" align="left" color="c3c3c3">
		
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-120416137-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-120416137-1');
</script>

            
</font></td></tr></tbody></table>
<!-- <iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div> -->

<!-- UNCOMMENT THE FOLLOWING TO GIVE CREDIT: -->
<!--  <p><font style="font-size: 11pt;" color="c1c1c1"> <center><a href='https://prashnani.github.io' style="color:c3c3c3">
website template credit: https://prashnani.github.io</a></center></font></p>  -->
</body></html>

